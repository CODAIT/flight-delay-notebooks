{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025086,
     "end_time": "2020-11-18T10:42:42.222104",
     "exception": false,
     "start_time": "2020-11-18T10:42:42.197018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Merging Airline Delay and Weather Datasets\n",
    "\n",
    "In this notebook, we merge together two data sources in order to create richer features for our flight delay prediction classification problem.\n",
    "* selecting the columns we wish to keep for later analysis\n",
    "* converting and cleaning data where required\n",
    "* handling missing values\n",
    "\n",
    "#### Import required modules\n",
    "\n",
    "Import and configure the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.039234,
     "end_time": "2020-11-18T10:42:42.285141",
     "exception": false,
     "start_time": "2020-11-18T10:42:42.245907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pandas scikit-learn > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.83582,
     "end_time": "2020-11-18T10:42:43.144936",
     "exception": false,
     "start_time": "2020-11-18T10:42:42.309116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define required imports\n",
    "import pandas as pd\n",
    "# These set pandas max column and row display in the notebook\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011064,
     "end_time": "2020-11-18T10:42:43.173214",
     "exception": false,
     "start_time": "2020-11-18T10:42:43.162150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read datasets\n",
    "\n",
    "We start by reading in the processed flight delay and weather datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.083716,
     "end_time": "2020-11-18T10:42:43.270535",
     "exception": false,
     "start_time": "2020-11-18T10:42:43.186819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flight_path = 'data/jfk_flight_features.csv'\n",
    "flight_data = pd.read_csv(flight_path, parse_dates=['flight_date'])\n",
    "flight_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.253188,
     "end_time": "2020-11-18T10:42:43.545450",
     "exception": false,
     "start_time": "2020-11-18T10:42:43.292262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_path = 'data/jfk_weather_features.csv'\n",
    "weather_data = pd.read_csv(weather_path, parse_dates=['DATE'])\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012112,
     "end_time": "2020-11-18T10:42:43.568539",
     "exception": false,
     "start_time": "2020-11-18T10:42:43.556427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Merge datasets\n",
    "\n",
    "The next step is to merge or join the two datasets, such that for each flight record in the flight delay dataset, we have information about the weather conditions present for that flight. \n",
    "\n",
    "**Note** we have to be careful not to effectively \"leak\" information. Recall that our weather observations come from automated weather station reports that are generated on the 51st minute of each hour. We must ensure that the weather report used for flight delay prediction is one covering weather conditions present _before_ the flight departure, otherwise we would be giving our model a glimpse in the the future!\n",
    "\n",
    "This makes joining the datasets a little tricky. One simple approach is to join the record for a given flight day and hour, with the weather reading for the same day but the _previous hour_. We can do this by extracting 2 \"join keys\" from each dataset: the first for the `date` and the second for the `hour` of the record. If we set the `hour` join key for the flight to the hour _before_ the actual hour of the flight scheduled departure, then we ensure the corresponding weather report comes from the hour before the flight would depart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.106487,
     "end_time": "2020-11-18T10:42:43.699995",
     "exception": false,
     "start_time": "2020-11-18T10:42:43.593508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flight_data.loc[:, 'hour_key'] = pd.to_datetime(flight_data['sched_dep_time'], format='%H%M', errors='ignore').dt.hour - 1\n",
    "flight_data.loc[:, 'date_key'] = flight_data['flight_date'].dt.date\n",
    "flight_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.117894,
     "end_time": "2020-11-18T10:42:43.842588",
     "exception": false,
     "start_time": "2020-11-18T10:42:43.724694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_data.loc[:, 'date_key'] = weather_data['DATE'].dt.date\n",
    "weather_data.loc[:, 'hour_key'] = weather_data['DATE'].dt.hour\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013882,
     "end_time": "2020-11-18T10:42:43.873685",
     "exception": false,
     "start_time": "2020-11-18T10:42:43.859803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we join the datasets together based on the \"join keys\" we have created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.104916,
     "end_time": "2020-11-18T10:42:43.991593",
     "exception": false,
     "start_time": "2020-11-18T10:42:43.886677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flight_weather_data = flight_data.merge(weather_data, how='inner', on=['date_key', 'hour_key'])\n",
    "flight_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016437,
     "end_time": "2020-11-18T10:42:44.022539",
     "exception": false,
     "start_time": "2020-11-18T10:42:44.006102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the first record in our flight dataset, we can see that the flight departs at 15:25. The corresponding weather report is timestamped at 14:51.\n",
    "\n",
    "**Note** all we guarantee here is that the weather report is _within_ 1 hour before the flight departure, not _precisely 1 hour before_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015937,
     "end_time": "2020-11-18T10:42:44.052584",
     "exception": false,
     "start_time": "2020-11-18T10:42:44.036647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save the Merged Data\n",
    "\n",
    "Finally, we save the merged dataset for use by downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.367411,
     "end_time": "2020-11-18T10:42:44.439419",
     "exception": false,
     "start_time": "2020-11-18T10:42:44.072008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flight_weather_data.to_csv('data/jfk_flight_weather_features.csv', index=False, float_format='%g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013914,
     "end_time": "2020-11-18T10:42:44.470581",
     "exception": false,
     "start_time": "2020-11-18T10:42:44.456667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"authors\"></a> \n",
    "### Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029833,
     "end_time": "2020-11-18T10:42:44.522443",
     "exception": false,
     "start_time": "2020-11-18T10:42:44.492610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook was created by the [Center for Open-Source Data & AI Technologies](http://codait.org).\n",
    "\n",
    "Copyright Â© 2019 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "duration": 5.066455,
   "end_time": "2020-11-18T10:42:45.841585",
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/nick/workspace/python/flight-delay-notebooks/notebooks/merge_data.ipynb",
   "output_path": "/Users/nick/workspace/python/flight-delay-notebooks/notebooks/merge_data.ipynb",
   "parameters": {},
   "start_time": "2020-11-18T10:42:40.775130",
   "version": "2.1.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
