{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057903,
     "end_time": "2020-11-18T10:42:27.967529",
     "exception": false,
     "start_time": "2020-11-18T10:42:27.909626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Processing NOAA Weather Data of JFK Airport (New York)\n",
    "\n",
    "This notebook relates to the NOAA Weather Dataset - JFK Airport (New York). The dataset contains 114,546 hourly observations of 12 local climatological variables (such as temperature, wind speed and precipitation) collected at JFK airport. This dataset is freely available from the IBM Developer [Data Asset Exchange](https://developer.ibm.com/exchanges/data/all/jfk-weather-data/).\n",
    "\n",
    "In this notebook, we process the raw dataset by:\n",
    "* selecting the columns we wish to keep for later downstream tasks\n",
    "* converting and cleaning data where required\n",
    "* filling missing values\n",
    "* extracting categorical weather features\n",
    "\n",
    "#### Import required modules\n",
    "\n",
    "Import and configure the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.072604,
     "end_time": "2020-11-18T10:42:28.096347",
     "exception": false,
     "start_time": "2020-11-18T10:42:28.023743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pandas > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.236463,
     "end_time": "2020-11-18T10:42:29.391378",
     "exception": false,
     "start_time": "2020-11-18T10:42:28.154915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# These set pandas max column and row display in the notebook\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053432,
     "end_time": "2020-11-18T10:42:29.492354",
     "exception": false,
     "start_time": "2020-11-18T10:42:29.438922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read the Raw Data\n",
    "\n",
    "We start by reading in the raw dataset, displaying the first few rows of the dataframe, and taking a look at the columns and column types present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.248616,
     "end_time": "2020-11-18T10:42:31.818019",
     "exception": false,
     "start_time": "2020-11-18T10:42:29.569403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/noaa-weather-data-jfk-airport/jfk_weather.csv', parse_dates=['DATE'])\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047672,
     "end_time": "2020-11-18T10:42:31.916277",
     "exception": false,
     "start_time": "2020-11-18T10:42:31.868605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Clean the Data\n",
    "\n",
    "As you can see above, there are a lot of fields which are non-numerical - usually these will be fields that contain text or categorical data, e.g. `HOURLYPRSENTWEATHERTYPE`.\n",
    "\n",
    "There are also fields, such as `HOURLYVISIBILITY`, that we may expect to be numerical, but are instead `object` type. This often indicates that there may be missing (or `null`) values, or some other unusual readings that we may have to deal with (since otherwise the field would have been fully parsed as a numerical data type).\n",
    "\n",
    "In addition, some fields relate to hourly observations, while others relate to daily or monthly intervals. For purposes of later analysis, we will restrict the dataset to a certain subset  of fields that relate to hourly observations.\n",
    "\n",
    "In this section, we refer to the [NOAA Local Climatological Data Documentation](https://data.noaa.gov/dataset/dataset/u-s-local-climatological-data-lcd/resource/ee7381ea-647a-434f-8cfa-81202b9b4c05) to describe the fields and meaning of various values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036204,
     "end_time": "2020-11-18T10:42:31.987330",
     "exception": false,
     "start_time": "2020-11-18T10:42:31.951126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Select data subset\n",
    "\n",
    "First, we select only the subset of data of interest. We will keep data for years 2010 - 2017 related to routine hourly weather station reports. We will also restrict our dataset to only a subest of column types that we expect to be pertinent for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.213779,
     "end_time": "2020-11-18T10:42:32.244195",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.030416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose what columns to import from raw data\n",
    "column_subset = [\n",
    "    'DATE',\n",
    "    'HOURLYVISIBILITY',\n",
    "    'HOURLYPRSENTWEATHERTYPE',\n",
    "    'HOURLYWindSpeed',\n",
    "    'HOURLYWindGustSpeed',\n",
    "    'HOURLYPrecip'\n",
    "]\n",
    "\n",
    "# Select the data sub-set for years 2010-2017 & report type FM-15 (routine hourly weather reports)\n",
    "data_subset = raw_data[(raw_data['DATE'].dt.year.isin(range(2010, 2018))) & (raw_data['REPORTTPYE'] == 'FM-15')]\n",
    "# Filter dataset to relevant columns\n",
    "weather_data = data_subset.loc[:, column_subset]\n",
    "# Set date index\n",
    "weather_data = weather_data.set_index(pd.DatetimeIndex(weather_data['DATE']))\n",
    "weather_data.drop(['DATE'], axis=1, inplace=True)\n",
    "weather_data.replace(to_replace='*', value=np.nan, inplace=True)\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.052686,
     "end_time": "2020-11-18T10:42:32.344763",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.292077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038565,
     "end_time": "2020-11-18T10:42:32.425397",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.386832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Clean up precipitation column\n",
    "\n",
    "From the dataframe preview above, we can see that the column `HOURLYPrecip` - which is the hourly measure of precipitation levels - contains both `NaN` and `T` values. `T` specifies *trace amounts of precipitation*, while `NaN` means *not a number*, and is used to denote missing values.\n",
    "\n",
    "We can also inspect the unique values present for the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.046581,
     "end_time": "2020-11-18T10:42:32.509475",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.462894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_data['HOURLYPrecip'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036444,
     "end_time": "2020-11-18T10:42:32.573140",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.536696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see that some values end with an `s` (indicating snow), while there is a strange value `0.020.01s` which appears to be an error of some sort. To deal with `T` values, we will set the observation to be `0`. We will also replace the erroneous value `0.020.01s` with `NaN`.\n",
    "\n",
    "Finally, we will replace all `NaN` entries with `0`, i.e. we assume no precipitation was present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.047662,
     "end_time": "2020-11-18T10:42:32.657348",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.609686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix precipitation data\n",
    "weather_data['HOURLYPrecip'].replace(to_replace='T', value='0.00', inplace=True)\n",
    "weather_data['HOURLYPrecip'].replace('0.020.01s', np.nan, inplace=True)\n",
    "weather_data.fillna(value={'HOURLYPrecip': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0353,
     "end_time": "2020-11-18T10:42:32.720877",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.685577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Inspect visibility column\n",
    "\n",
    "As we have done for precipitation, we can also inspect the unique values present for the column `HOURLYVISIBILITY` - which is the hourly measure of visibility. Below, we see that some values are `nan`, while some end with an `V`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.047845,
     "end_time": "2020-11-18T10:42:32.796248",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.748403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_data['HOURLYVISIBILITY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033404,
     "end_time": "2020-11-18T10:42:32.875565",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.842161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Convert columns to numerical types\n",
    "\n",
    "Next, we will convert string columns that refer to numerical values to numerical types. For columns such as `HOURLYPrecip` and `HOURLYVISIBILITY`, we first also drop the non-numerical parts of the value (e.g .the `s` and `V` characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.612421,
     "end_time": "2020-11-18T10:42:33.517136",
     "exception": false,
     "start_time": "2020-11-18T10:42:32.904715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set of columns to convert\n",
    "messy_columns = ['HOURLYVISIBILITY', 'HOURLYPrecip', 'HOURLYWindSpeed', 'HOURLYWindGustSpeed']\n",
    "\n",
    "# Convert columns to float32 datatype\n",
    "for i in messy_columns:\n",
    "    weather_data[i] = weather_data[i].apply(lambda x: re.sub('[^0-9,.-]', '', x) if type(x) == str else x).replace('', np.nan).astype(('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02799,
     "end_time": "2020-11-18T10:42:33.571211",
     "exception": false,
     "start_time": "2020-11-18T10:42:33.543221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can now see that all fields have numerical data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.152779,
     "end_time": "2020-11-18T10:42:33.793868",
     "exception": false,
     "start_time": "2020-11-18T10:42:33.641089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(weather_data.info())\n",
    "# Generate the summary statistics for each column\n",
    "weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047737,
     "end_time": "2020-11-18T10:42:33.890040",
     "exception": false,
     "start_time": "2020-11-18T10:42:33.842303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For wind gusts, rather than have `NaN` entries (which represent no gusts), we will represent the gust speed column as \"excess speed\" over the `HOURLYWindSpeed` values.weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.276195,
     "end_time": "2020-11-18T10:42:34.197609",
     "exception": false,
     "start_time": "2020-11-18T10:42:33.921414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_data.loc[:, 'HOURLYWindGustSpeed'] = np.vectorize(lambda x, y: 0.0 if np.isnan(y) else y - x)(\n",
    "    weather_data['HOURLYWindSpeed'], weather_data['HOURLYWindGustSpeed'])\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034999,
     "end_time": "2020-11-18T10:42:34.262944",
     "exception": false,
     "start_time": "2020-11-18T10:42:34.227945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check date index\n",
    "\n",
    "Next, we check if there are any duplicates with respect to our `DATE` index and check furthermore that our dates are in the correct order (that is, strictly increasing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.054207,
     "end_time": "2020-11-18T10:42:34.353374",
     "exception": false,
     "start_time": "2020-11-18T10:42:34.299167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cond = len(weather_data[weather_data.index.duplicated()].sort_index())\n",
    "print('Date index contains no duplicate entries: {}'.format(cond == 0))\n",
    "print('Date index is strictly increasing: {}'.format(weather_data.index.is_monotonic_increasing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032104,
     "end_time": "2020-11-18T10:42:34.417847",
     "exception": false,
     "start_time": "2020-11-18T10:42:34.385743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Categorical Feature Extraction\n",
    "\n",
    "The final pre-processing step we will perform will be to handle the `HOURLYPRSENTWEATHERTYPE` column to correctly encode the weather features. This column indicates the presence of specific weather types for the given reading. For example, `-RA:02 BR:1 |RA:61 |RA:61` refers to 3 types of reading:\n",
    "1. `AU` codes for automated weather readings\n",
    "2. `AW` codes for a different type of automated weather reading\n",
    "3. `MW` codes for manually-augmented weather readings\n",
    "\n",
    "This example reading happens to contain all 3 types, separated by a `|` character. The `AU` code is thus `-RA:02 BR:1`. If we refer to the data documentation linked above, we can see this indicates the presence of `RA:02 - Rain` and `BR:1 - Mist`.\n",
    "\n",
    "These _present weather types_ are categorical variables. **Note** that multiple categories of weather can be present. In order to process this column, we will:\n",
    "* only use the `AU` codes for simplicity\n",
    "* convert the codes to more readable category names\n",
    "* extract the weather type categories into individual binary columns representing the presence (`1`) or absence (`0`) of that category. This is like \"one-hot encoding\" but for multi-category variables\n",
    "\n",
    "We start with creating a mapping from codes to category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.05096,
     "end_time": "2020-11-18T10:42:34.509497",
     "exception": false,
     "start_time": "2020-11-18T10:42:34.458537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start with raw types taken from the LCD Dataset Documentation\n",
    "# we convert the raw weather type names to snake_case\n",
    "raw_types = '''DZ:01 - drizzle\n",
    "RA:02 - rain\n",
    "SN:03 - snow\n",
    "SG:04 - snow_grains\n",
    "IC:05 - ice_crystals\n",
    "PL:06 - ice_pellets\n",
    "GR:07 - hail\n",
    "GS:08 - small_hail_snow_pellets\n",
    "UP:09 - unknown_precipitation\n",
    "BR:1 - mist\n",
    "FG:2 - fog\n",
    "FU:3 - smoke\n",
    "VA:4 - volcanic_ash\n",
    "DU:5 - widespread_dust\n",
    "SA:6 - sand\n",
    "HZ:7 - haze\n",
    "PY:8 - spray\n",
    "PO:1 - well_developed_dust\n",
    "SQ:2 - squalls\n",
    "FC:3 - funnel_cloud_waterspout_tornado\n",
    "SS:4 - sandstorm\n",
    "DS:5 - duststorm'''.split('\\n')\n",
    "\n",
    "raw_types = [t.split(' - ') for t in raw_types]\n",
    "weather_types = {t[0]: t[1] for t in raw_types}\n",
    "# Add in a code that is inconsistently represented in the documentation\n",
    "weather_types['TS:7'] = 'thunderstorm'\n",
    "weather_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052291,
     "end_time": "2020-11-18T10:42:34.593200",
     "exception": false,
     "start_time": "2020-11-18T10:42:34.540909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are still a few edge cases that do not fall within the weather type mapping we have created. For the purposes of simplification, we will ignore these, since we have captured the main weather types in our mapping. So, we create a function to convert codes to category names, handling any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.074268,
     "end_time": "2020-11-18T10:42:34.731431",
     "exception": false,
     "start_time": "2020-11-18T10:42:34.657163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_type(k):\n",
    "    if k in weather_types:\n",
    "        return weather_types[k]\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def extract_weather_type(x):\n",
    "    wt = x.split('|')[0].split() if isinstance(x, str) else []\n",
    "    wt = [get_type(w.lstrip('-').lstrip('+')) for w in wt]\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068491,
     "end_time": "2020-11-18T10:42:34.864111",
     "exception": false,
     "start_time": "2020-11-18T10:42:34.795620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's test our function out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.224131,
     "end_time": "2020-11-18T10:42:35.139554",
     "exception": false,
     "start_time": "2020-11-18T10:42:34.915423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(weather_data['HOURLYPRSENTWEATHERTYPE'].head(5))\n",
    "print()\n",
    "print(weather_data['HOURLYPRSENTWEATHERTYPE'].apply(extract_weather_type).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.054716,
     "end_time": "2020-11-18T10:42:35.260798",
     "exception": false,
     "start_time": "2020-11-18T10:42:35.206082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That seems to be working. Next, we binarize the present weather categories in each cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.820289,
     "end_time": "2020-11-18T10:42:36.111270",
     "exception": false,
     "start_time": "2020-11-18T10:42:35.290981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = weather_data['HOURLYPRSENTWEATHERTYPE'].apply(extract_weather_type).apply(Counter)\n",
    "counts = pd.DataFrame.from_records(counts).fillna(value=0).drop(columns = [''])\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053868,
     "end_time": "2020-11-18T10:42:36.209831",
     "exception": false,
     "start_time": "2020-11-18T10:42:36.155963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, we combine the extra columns we've created with our original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.115044,
     "end_time": "2020-11-18T10:42:36.363452",
     "exception": false,
     "start_time": "2020-11-18T10:42:36.248408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_data = pd.concat([weather_data, counts.set_index(weather_data.index)], axis=1)\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053257,
     "end_time": "2020-11-18T10:42:36.454488",
     "exception": false,
     "start_time": "2020-11-18T10:42:36.401231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Rename columns\n",
    "\n",
    "Before saving the dataset, we will rename the columns for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.065562,
     "end_time": "2020-11-18T10:42:36.566875",
     "exception": false,
     "start_time": "2020-11-18T10:42:36.501313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.079686,
     "end_time": "2020-11-18T10:42:36.723515",
     "exception": false,
     "start_time": "2020-11-18T10:42:36.643829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define some new column names for consistency\n",
    "columns_name_map = {\n",
    "    'HOURLYVISIBILITY': 'visibility',\n",
    "    'HOURLYPRSENTWEATHERTYPE': 'weather_type_raw',\n",
    "    'HOURLYWindSpeed': 'wind_speed',\n",
    "    'HOURLYWindGustSpeed': 'wind_gust_speed',\n",
    "    'HOURLYPrecip': 'precip',\n",
    "}\n",
    "\n",
    "cleaned_data_renamed = cleaned_data.rename(columns=columns_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.143285,
     "end_time": "2020-11-18T10:42:36.925259",
     "exception": false,
     "start_time": "2020-11-18T10:42:36.781974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cleaned_data_renamed.info())\n",
    "print()\n",
    "cleaned_data_renamed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.161798,
     "end_time": "2020-11-18T10:42:37.164530",
     "exception": false,
     "start_time": "2020-11-18T10:42:37.002732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log some general information about the dataset\n",
    "print('# of columns: ' + str(cleaned_data_renamed.shape[1])) \n",
    "print('# of observations: ' + str(cleaned_data_renamed.shape[0]))\n",
    "print('Start date: ' + str(cleaned_data_renamed.index[0]))\n",
    "print('End date: ' + str(cleaned_data_renamed.index[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.109368,
     "end_time": "2020-11-18T10:42:37.386139",
     "exception": false,
     "start_time": "2020-11-18T10:42:37.276771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save the Processed Data\n",
    "\n",
    "Finally, we save the processed dataset for use by downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.452826,
     "end_time": "2020-11-18T10:42:39.912468",
     "exception": false,
     "start_time": "2020-11-18T10:42:37.459642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_data_renamed.to_csv('data/jfk_weather_features.csv', float_format='%g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049907,
     "end_time": "2020-11-18T10:42:40.004559",
     "exception": false,
     "start_time": "2020-11-18T10:42:39.954652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " ### Authors\n",
    " \n",
    " This notebook was created by the [Center for Open-Source Data & AI Technologies](http://codait.org).\n",
    "\n",
    "Copyright © 2020 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "duration": 14.833694,
   "end_time": "2020-11-18T10:42:40.587559",
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/nick/workspace/python/flight-delay-notebooks/notebooks/process_weather_data.ipynb",
   "output_path": "/Users/nick/workspace/python/flight-delay-notebooks/notebooks/process_weather_data.ipynb",
   "parameters": {},
   "start_time": "2020-11-18T10:42:25.753865",
   "version": "2.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
